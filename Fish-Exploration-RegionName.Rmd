---
title: "Fish-Exploration-RegionName"
author: "Lewei Er"
date: "2023-11-30"
output: html_document
---

```{r setup, include=FALSE}
library(dplyr)
library(tidyr)
library(lubridate)
# put the data in the same folder with the project
load("~/Ontario-Lake-Fish-Classification/processed_AnalysisData.Rdata")

processed_data%>%group_by(spCode,fishNum)%>%count()

# Remove individuals with missing transducers
processed_data<-processed_data%>%filter(is.na(F100)==F)

# also remove individual LWF23018 (only two pings)
processed_data<-processed_data%>%filter(fishNum!="LWF23018")

# remove the individual with -9x10^38 TS
processed_data<-processed_data[-36830,]

## Lake Trout Filtering
## LT 009, 010, 012, 014 (although looks more like a dead fish), 016, 017 (6/10 condition), 021, 23007, 23009, 23013, 23012, 23011, 23010, 23005, 23004, 23003, 23002 are all good and were behaving normally.

# LT019 and LT23008 were dead the whole time. Remove. 
processed_data<-processed_data%>%filter(fishNum!="LT019")
processed_data<-processed_data%>%filter(fishNum!="LT23008")

# LT015, 018, and 23018 were bad on the way down and then good on retrieval - will filter the first parts of the data

# LT015 was at a very shallow depth and on quadrant for the majority of pinging, then seemed to move to the correct depth. Keep only those pings
processed_data <- processed_data[!(processed_data$fishNum == "LT015" & processed_data$Target_true_depth > 15.5 ) ,]

# LT018 was rough on attachment too, also has two rythmic changes in depth - going to remove the times the fish was above 15.5m as this will get rid of the time that fish was being dragged potentially and the first part of the timeseries where the fish was rough
processed_data <- processed_data[!(processed_data$fishNum == "LT018" & processed_data$Target_true_depth > 15.5 ) ,]

# LT23018 was rough on attachment, barely alive, but "suprisingly okay" coming back - no clear indication of when it got "okay" in the data so remove whole fish
processed_data<-processed_data%>%filter(fishNum!="LT23018")


# LT013, and LT23001 went down okay but died sometime down there. 

# LT013 was almost dead on release as well as dead on retrival - remove all. 
processed_data<-processed_data%>%filter(fishNum!="LT013")

# LT23001 looks like there are barely any salvagable pings - remove all
processed_data<-processed_data%>%filter(fishNum!="LT23001")


## Cleaned data: 
processed_data%>%filter(spCode==81)%>%group_by(fishNum)%>%count()
# 21 fish, 22,792 pings

## Looking at cleaned LT data
print(processed_data%>%filter(spCode==81)%>%group_by(fishNum)%>%summarise(TL=mean(totalLength)),n=21)


## Lake Whitefish Filtering
## LWF 004, 005, 006, 007,010,012, 013, 014, 015, 23001, 23002, 23003, 23005, 23007, 23009, 23010, 23011, 23013, 23015, 23016, 23017 are all good and behaving normally.

# 23004 and 23014 both pretty much dead the whole time
processed_data<-processed_data%>%filter(fishNum!="LWF23004")
processed_data<-processed_data%>%filter(fishNum!="LWF23014")


# 23006 and 23008 both "swam" upside down for the majority of the time
processed_data<-processed_data%>%filter(fishNum!="LWF23006")
processed_data<-processed_data%>%filter(fishNum!="LWF23008")


# LWF011, sounder on at 8:11, remove pings between 8:09 and 8:13.

LWF011<-processed_data%>%
        filter(fishNum=="LWF011")%>%
        mutate(Ping_time=strptime(Ping_time,format = "%H:%M:%S")-(60*60*4), hour=hour(Ping_time), minute=minute(Ping_time))%>%
  filter(hour==19 | hour==20 & minute <= 9 | hour==20 & minute >= 13)%>%select(c(-hour,-minute))

processed_data<-processed_data%>%filter(fishNum!="LWF011")

processed_data<-rbind(processed_data,LWF011)


# LWF23012 was held at a different depth for the full time, LWF009 was in poor condition on retrieval but activity score 3: keep for now but keep in mind.
```

```{r}
# make plots for depth over time
fishes <- unique(processed_data$Region_name)

for (fish in fishes) {
  plot(processed_data[processed_data$Region_name == fish,]$pingNumber, -processed_data[processed_data$Region_name == fish,]$Target_true_depth, main = fish)
}
```

```{r}
# keeping only 90, 62, 101, 84, 85
fish_data <- processed_data[processed_data$Region_name == "Region_90" | processed_data$Region_name == "Region_62" | processed_data$Region_name == "Region_101" | processed_data$Region_name == "Region_84" | processed_data$Region_name == "Region_85",]
```

```{r}
library(reticulate)
Sys.setenv(RETICULATE_PYTHON = "/Applications/Python 3.9")
library(keras)
#install_keras()


# fomulate multivariate time series data and species codes
time_series_data <- fish_data[, grep("F45|F260", colnames(fish_data), value = TRUE)]
species <- fish_data$spCode

# data normalization
time_series_data <- scale(time_series_data)

# convert species to categorical
species <- to_categorical(as.integer(species))

# Split data into training and testing sets
set.seed(123)
indices <- sample(1:nrow(time_series_data), size = 0.8 * nrow(time_series_data))
train_data <- time_series_data[indices, ]
test_data <- time_series_data[-indices, ]
train_labels <- species[indices, ]
test_labels <- species[-indices, ]

# Residual Block
residual_block <- function(input, filters, kernel_size, strides, use_activation = TRUE) {
  x <- input
  x <- layer_conv_1d(filters = filters, kernel_size = kernel_size, strides = strides, padding = "same")(x)
  x <- layer_batch_normalization()(x)
  if (use_activation) {
    x <- layer_activation("relu")(x)
  }
  x <- layer_conv_1d(filters = filters, kernel_size = kernel_size, strides = strides, padding = "same")(x)
  x <- layer_batch_normalization()(x)

  # Add the input (shortcut connection)
  x <- layer_add(list(x, input))
  if (use_activation) {
    x <- layer_activation("relu")(x)
  }
  return(x)
}

# ResNet Model
input_shape <- dim(train_data)[2]
output_classes <- dim(species)[2]

input <- layer_input(shape = c(input_shape, 1))
x <- layer_conv_1d(filters = 64, kernel_size = 7, strides = 1, padding = "same")(input)
x <- layer_batch_normalization()(x)
x <- layer_activation("relu")(x)

# add residual blocks
x <- residual_block(x, filters = 64, kernel_size = 3, strides = 1)
x <- residual_block(x, filters = 64, kernel_size = 3, strides = 1)

# output part of the model
x <- layer_global_average_pooling_1d()(x)
output <- layer_dense(units = output_classes, activation = "softmax")(x)

model <- keras_model(inputs = input, outputs = output)

# compile the model
model %>% compile(
  loss = "categorical_crossentropy",
  optimizer = optimizer_adam(),
  metrics = c("accuracy")
)

# train the model
history <- model %>% fit(
  train_data, train_labels,
  epochs = 20,
  batch_size = 32,
  validation_split = 0.2
)

# evaluate the model
model %>% evaluate(test_data, test_labels)
```


