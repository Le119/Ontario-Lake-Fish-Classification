## Jess ResNet, based on Lewei & Rita's work

## Load in the necessary libraries
library(dplyr)
library(tidyr)
library(keras)
library(rBayesianOptimization)
library(tidymodels)
library(caret)
library(tensorflow)
library(torchvision)

envname="490env"
use_condaenv(envname, required = TRUE)

load("/Users/luoyihang/Downloads/processed_AnalysisData_no200.Rdata")

processed_data <- processed_data_no200

processed_data%>%group_by(spCode,fishNum)%>%count()

processed_data <- processed_data %>% filter(is.na(F100) == F)
processed_data <- processed_data %>% filter(fishNum != "LWF23018")
processed_data <- processed_data %>% select(-F90, -F90.5)
processed_data <- processed_data %>% filter(spCode == "81" | spCode == "316")
processed_data$species <- ifelse(processed_data$spCode == 81, "LT", "SMB")
processed_data <- processed_data %>% filter(is.na(aspectAngle) == F & is.na(Angle_major_axis) == F & is.na(Angle_minor_axis) == F)
processed_data <- processed_data %>% filter(F100 > -1000)

set.seed(73)
split <- group_initial_split(processed_data, group = fishNum, strata = species, prop = 0.7)
train <- training(split)
val_test <- testing(split)
split2 <- group_initial_split(val_test, group = fishNum, strata = species, prop = 0.5)
validate <- training(split2)
test <- testing(split2)

train%>%group_by(species)%>%count()
validate%>%group_by(species)%>%count()
test%>%group_by(species)%>%count()

train <- slice_sample(train, n = 6378, by = species)
validate <- slice_sample(validate, n = 1382, by = species)
test <- slice_sample(test, n = 1374, by = species)

train%>%group_by(species)%>%count()
validate%>%group_by(species)%>%count()
test%>%group_by(species)%>%count()

train$y <- ifelse(train$species == "LT", 0, 1)
dummy_y_train <- to_categorical(train$y, num_classes = 2)
test$y <- ifelse(test$species == "LT", 0, 1)
dummy_y_test <- to_categorical(test$y, num_classes = 2)
validate$y <- ifelse(validate$species == "LT", 0, 1)
dummy_y_val <- to_categorical(validate$y, num_classes = 2)


x_train <- train %>% select(52:300)
x_train<-x_train+10*log10(450/train$totalLength)
x_train<-exp(x_train/10)
x_train<-x_train%>%scale()
x_train<-as.matrix(x_train)

xmean<-attributes(x_train)$`scaled:center`
xsd<-attributes(x_train)$`scaled:scale`
x_test <- test %>% select(52:300)
x_test<-x_test+10*log10(450/test$totalLength)
x_test<-exp(x_test/10)
x_test<-x_test%>%scale(xmean,xsd)
x_test<-as.matrix(x_test)

x_validate <- validate %>% select(52:300)
x_validate<-x_validate+10*log10(450/validate$totalLength)
x_validate<-exp(x_validate/10)
x_validate<-x_validate%>%scale(xmean,xsd)
x_validate<-as.matrix(x_validate)

# Shuffle training data
# Assuming you have your data in variables `x_train` and `dummy_y_train`
# Select a subset of your data
set.seed(250)
subset_indices <- sample(1:nrow(x_train), size = 20)
x_train_subset <- x_train[subset_indices, ]
dummy_y_train_subset <- dummy_y_train[subset_indices, ]

# create grid of parameter space we want to search
regrate<-c(1e-6,1e-5,1e-4)
lstmunits<-c(256,128,64)
neuron1<-c(256,128,64,32,16)

# expand the grid so that every possible combination of the above parameters is present. 
grid.search.full<-expand.grid(regrate=regrate,lstmunits=lstmunits,neuron1=neuron1)
set.seed(15)
x <- sample(1:45, 20, replace=F)
grid.search.subset <- grid.search.full[x,]

# Number of folds
k <- 5
folds <- sample(1:k, size = nrow(x_train_subset), replace = TRUE)

# Store performance measure for each fold
val_loss <- matrix(nrow=20, ncol=5)
best_epoch <- matrix(nrow=20, ncol=5)

for(i in 1:20){
  for(fold in 1:5){
    fold_index <- which(folds == fold)
    x_train_set <- x_train_subset[-fold_index, ]
    y_train_set <- dummy_y_train_subset[-fold_index, ]
    x_val_set <- x_train_subset[fold_index, ]
    y_val_set <- dummy_y_train_subset[fold_index, ]
    set_random_seed(15)
    # Define the model (as you did in your code)
    inputs <- layer_input(shape = c(249,1))
    block_1_output <- inputs %>%
    layer_conv_1d(filters = 16, kernel_size = 6, activation = "relu", padding = "same",strides = 1)
    block_2_output <- block_1_output %>%
    layer_conv_1d(16, 6, activation = "relu", padding = "same") %>%
    layer_conv_1d(16, 6, activation = "relu", padding = "same") %>%
    layer_add(block_1_output)
    block_3_output <- block_2_output %>%
    layer_conv_1d(16, 6, activation = "relu", padding = "same") %>%
    layer_conv_1d(16, 6, activation = "relu", padding = "same") %>%
    layer_add(block_2_output)
    block_4_output <- block_3_output %>%
    layer_conv_1d(16, 6, activation = "relu", padding = "same") %>%
    layer_conv_1d(16, 6, activation = "relu", padding = "same") %>%
    layer_add(block_3_output)
    block_5_output <- block_4_output %>%
    layer_conv_1d(16, 6, activation = "relu", padding = "same") %>%
    layer_conv_1d(16, 6, activation = "relu", padding = "same") %>%
    layer_add(block_4_output)
    outputs <- block_5_output %>%
    layer_conv_1d(16, 6, activation = "relu", padding = "same") %>%
    layer_conv_1d(16, 6, activation = "relu", padding = "same") %>%
    layer_max_pooling_1d(pool_size = 2) %>%
    layer_flatten()%>%
    layer_dense(2, activation="sigmoid")
    model <- keras_model(inputs, outputs)
    model %>% compile(
      optimizer = optimizer_adam(lr = grid.search.subset$regrate[i]),
      loss = "binary_crossentropy",
      metrics = c("accuracy")
    )
    history <- model %>% fit(
      x_train_set,
      y_train_set,
      epochs = 30,
      batch_size = 32,
      validation_data = list(x_val_set, y_val_set)
    )
    best_epoch[i, fold] <- which.min(history$metrics$val_loss)
    val_loss[i, fold] <- min(history$metrics$val_loss)
  }
}

# find the lowest validation loss 
which(val_loss==min(val_loss),arr.ind = T)
lowest_val_loss=which(val_loss==min(val_loss),arr.ind = T) # name so can call on it later
# val_loss[5,4]
# best_epoch[5,4]
# the overall lowest validation loss was 0.418, but it occurred in epoch 29/30, i.e. the model could have improved more.

val_loss[lowest_val_loss] # row num must be what was outputted from prev line that finds the lowest val loss
best_epoch[lowest_val_loss]

# find best mean val loss
which(rowMeans(val_loss)==min(rowMeans(val_loss)))
best_mean_val_loss=which(rowMeans(val_loss)==min(rowMeans(val_loss)))
mean(val_loss[best_mean_val_loss[1],])
mean(best_epoch[best_mean_val_loss[1],])
val_loss[best_mean_val_loss]      
best_epoch[best_mean_val_loss]

fold = 1
fold_index<-which(folds==fold)
x_train_set<-x_train[-fold_index,]
y_train_set<-dummy_y_train[-fold_index]

x_val_set<-x_train[fold_index,]
y_val_set<-dummy_y_train[fold_index]

set_random_seed(15)
input_shape <- c(249,1)
inputs <- layer_input(shape = input_shape)

block_1_output <- inputs %>%
  layer_conv_1d(filters = 16, kernel_size = 6, activation = "relu", padding = "same",strides = 1)

block_2_output <- block_1_output %>%
  layer_conv_1d(16, 6, activation = "relu", padding = "same") %>%
  layer_conv_1d(16, 6, activation = "relu", padding = "same") %>%
  layer_add(block_1_output)
  
block_3_output <- block_2_output %>%
  layer_conv_1d(16, 6, activation = "relu", padding = "same") %>%
  layer_conv_1d(16, 6, activation = "relu", padding = "same") %>%
  layer_add(block_2_output)

block_4_output <- block_3_output %>%
  layer_conv_1d(16, 6, activation = "relu", padding = "same") %>%
  layer_conv_1d(16, 6, activation = "relu", padding = "same") %>%
  layer_add(block_3_output)

block_5_output <- block_4_output %>%
  layer_conv_1d(16, 6, activation = "relu", padding = "same") %>%
  layer_conv_1d(16, 6, activation = "relu", padding = "same") %>%
  layer_add(block_4_output)

outputs <- block_5_output %>%
  layer_conv_1d(16, 6, activation = "relu", padding = "same") %>%
  layer_conv_1d(16, 6, activation = "relu", padding = "same") %>%
  layer_max_pooling_1d(pool_size = 2) %>%
  layer_flatten()%>%
  layer_dense(2, activation="sigmoid")

model <- keras_model(inputs, outputs)
model
model %>% compile(
  optimizer = optimizer_adam(),
  loss = loss_categorical_crossentropy,
  metrics = c("accuracy")
)

resnet_history <- model %>% fit(
  x_train,dummy_y_train,
  batch_size = 1000,
  epochs = 75,
  validation_data = list(x_validate, dummy_y_val)
)

plot(resnet_history)
evaluate(model, (x_test), dummy_y_test)
preds <- predict(model, x_test)

species_predictions <- apply(preds, 1, which.max) - 1
species_predictions <- ifelse(species_predictions == 0, "LT", "SMB")
confusionMatrix(factor(species_predictions), factor(test$species))
