{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af0b8c01-a470-4e41-b2bd-974a75c74f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "(32954, 249)\n",
      "[14 18 17 32 13 33 20  8 36 28 29 10 44  3 22 48 30 35 24 23  4 42  7  1\n",
      " 49 41 45 15 46 16 34 37  0  5 21]\n",
      "[11  2 43  6 25 26 39]\n",
      "[27 12 31 47  9 40 38 19]\n",
      "(23208,)\n",
      "(5182,)\n",
      "(4564,)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle as pkl\n",
    "from argparse import ArgumentParser\n",
    "from copy import deepcopy\n",
    "from os.path import join as oj\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import configparser\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.utils.data\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import csv\n",
    "import optuna\n",
    "\n",
    "sys.path.insert(0, \"../1D CNN\")\n",
    "import utils\n",
    "import models\n",
    "import data_fns\n",
    "import my_eval\n",
    "\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed()\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"../config.ini\")\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def get_args():\n",
    "    parser = ArgumentParser(description=\"Functional group analysis\")\n",
    "\n",
    "    parser.add_argument(\"--exp_name\", type=str, default=\"\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=128)\n",
    "    parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
    "    parser.add_argument(\"--seed\", type=int, default=0)\n",
    "\n",
    "    parser.add_argument(\"--num_conv\", type=int, default=16)\n",
    "    parser.add_argument(\"--patience\", type=int, default=20)\n",
    "    ret_args, unknown = parser.parse_known_args()\n",
    "    return ret_args\n",
    "\n",
    "\n",
    "args = get_args()\n",
    "\n",
    "\n",
    "# data = np.load(\"data.npy\")\n",
    "# labels = np.load('labels.npy')\n",
    "\n",
    "# filters = ((\"state\", \"gas\"), (\"yunits\", \"ABSORBANCE\"), (\"xunits\", \"1/CM\"))\n",
    "# data, labels = utils.fixed_domain_filter(\n",
    "#     irdata, irindex, filters\n",
    "# )  # Filter points from the dataset\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "\n",
    "file_path = \"TS_freq_array_0208_new.csv\"\n",
    "save_path = \"C:\\\\Users\\\\aethe\\\\1D CNN\"\n",
    "# Initialize arrays\n",
    "y_labels = []\n",
    "x_TS = []\n",
    "fish_num = []\n",
    "final_labels = []\n",
    "\n",
    "# Read the CSV file line by line\n",
    "with open(file_path, 'r') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    \n",
    "    next(csv_reader)\n",
    "    \n",
    "    # Extract the first column and construct rows as arrays\n",
    "    for row in csv_reader:\n",
    "        fish_num.append(row[0])\n",
    "        y_labels.append(row[-2])\n",
    "        final_labels.append([row[-2],row[-1]])\n",
    "        x_TS.append(row[2:-2])\n",
    "\n",
    "\n",
    "x_TS = np.array(x_TS, dtype = float)\n",
    "\n",
    "print(x_TS.shape) # (32954, 249)\n",
    "\n",
    "means = np.mean(x_TS, axis=0)\n",
    "std_devs = np.std(x_TS, axis=0)\n",
    "x_TS = (x_TS - means) / std_devs  # Standardizing data\n",
    "\n",
    "\n",
    "# Contruct data and labels\n",
    "data = x_TS[:, None]\n",
    "labels = np.array(y_labels, dtype=int)\n",
    "final_labels = np.array(final_labels, dtype=int)\n",
    "\n",
    "# When spliting the dataset, we first split based on each fish to aviod data leakage\n",
    "# Create a mapping dictionary from fish_num to indices\n",
    "\n",
    "fish_num_to_index = {fish_num: index for index, fish_num in enumerate(np.unique(np.array(fish_num)))}\n",
    "\n",
    "# Use the mapping to convert the fish_num to indices\n",
    "fish_ind = np.array([fish_num_to_index[num] for num in fish_num])\n",
    "\n",
    "\n",
    "train_idxs_by_fish, val_idxs_by_fish, test_idxs_by_fish = data_fns.get_split(len(np.unique(fish_ind)), seed=23)\n",
    "\n",
    "# Checn index split by fish\n",
    "print(train_idxs_by_fish)\n",
    "print(val_idxs_by_fish)\n",
    "print(test_idxs_by_fish)\n",
    "\n",
    "# This function takes in the indices that split by individual fish\n",
    "# It then returns the indices split by each time ping correspond to the individual fish\n",
    "def construct_data_index_by_fish_idxs(idxs_split_by_fish, fish_ind):\n",
    "    indx = np.array([], dtype=int)  # Ensure the array is of integer type\n",
    "    for split_fish_ind in idxs_split_by_fish:\n",
    "        # Ensure split_fish_ind is an integer before using it as an index\n",
    "        split_fish_ind = int(split_fish_ind)\n",
    "        indx_current_fish = np.where(fish_ind == split_fish_ind)[0]\n",
    "        indx = np.append(indx, indx_current_fish)\n",
    "\n",
    "    return indx\n",
    "\n",
    "\n",
    "\n",
    "# Indices that we use in the model\n",
    "train_idxs = construct_data_index_by_fish_idxs(train_idxs_by_fish, fish_ind)\n",
    "val_idxs = construct_data_index_by_fish_idxs(val_idxs_by_fish, fish_ind)\n",
    "test_idxs = construct_data_index_by_fish_idxs(test_idxs_by_fish, fish_ind)\n",
    "\n",
    "\n",
    "\n",
    "print(train_idxs.shape) # (22925,)\n",
    "print(val_idxs.shape) # (4662,)\n",
    "print(test_idxs.shape) # (5367,)\n",
    "\n",
    "def balance_classes_by_indices(indices, labels, n_samples_per_class):\n",
    "    unique_classes = np.unique(labels[indices])\n",
    "    balanced_indices = np.array([], dtype=int)\n",
    "\n",
    "    for cls in unique_classes:\n",
    "        class_indices = indices[labels[indices] == cls]\n",
    "        if len(class_indices) > n_samples_per_class:\n",
    "            class_indices = np.random.choice(class_indices, n_samples_per_class, replace=False)\n",
    "        else:\n",
    "            class_indices = np.random.choice(class_indices, n_samples_per_class, replace=True)\n",
    "        balanced_indices = np.append(balanced_indices, class_indices)\n",
    "\n",
    "    np.random.shuffle(balanced_indices)  # Shuffle to mix classes\n",
    "    return balanced_indices\n",
    "\n",
    "# Apply balancing to each split\n",
    "balanced_train_idxs = balance_classes_by_indices(train_idxs, labels, n_samples_per_class=10029)\n",
    "balanced_val_idxs = balance_classes_by_indices(val_idxs, labels, n_samples_per_class=1000)\n",
    "balanced_test_idxs = balance_classes_by_indices(test_idxs, labels, n_samples_per_class=2000)\n",
    "\n",
    "\n",
    "#%%\n",
    "# num_classes = labels.shape[1]\n",
    "num_classes = 2\n",
    "# train_idxs, val_idxs, test_idxs = data_fns.get_split(len(data), seed=42)\n",
    "\n",
    "# scale data\n",
    "# weights = labels[train_idxs].mean(axis=0)\n",
    "# anti_weights = 1 - weights\n",
    "\n",
    "# mult_weights = weights * anti_weights\n",
    "# weights /= mult_weights\n",
    "# anti_weights /= mult_weights\n",
    "\n",
    "# weights = torch.tensor(weights).to(device)\n",
    "# anti_weights = torch.tensor(anti_weights).to(device)\n",
    "\n",
    "\n",
    "#%%\n",
    "# create datasets in torch\n",
    "torch.manual_seed(args.seed)\n",
    "train_dataset = TensorDataset(\n",
    "    *[torch.Tensor(input) for input in [data[balanced_train_idxs], final_labels[balanced_train_idxs]]],\n",
    ")\n",
    "val_dataset = TensorDataset(\n",
    "    *[torch.Tensor(input) for input in [data[balanced_val_idxs], final_labels[balanced_val_idxs]]],\n",
    ")\n",
    "\n",
    "test_dataset = TensorDataset(\n",
    "    *[torch.Tensor(input) for input in [data[balanced_test_idxs], final_labels[balanced_test_idxs]]],\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "print(num_classes)\n",
    "\n",
    "\n",
    "\n",
    "model = models.FGANet(\n",
    "    num_input=data.shape[2],\n",
    "    num_output=2,\n",
    "    conv_channels=args.num_conv,\n",
    "    num_in_channels=data.shape[1],\n",
    "    stride=1,\n",
    ").to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2477c9b-ff17-49d7-9231-b76a558679f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, TrLoss: 0.57113, testLoss: 0.68929, testAcc: 0.66\n",
      "Epoch: 2, TrLoss: 0.47610, testLoss: 0.71133, testAcc: 0.67\n",
      "Epoch: 3, TrLoss: 0.43326, testLoss: 0.71750, testAcc: 0.67\n",
      "Epoch: 4, TrLoss: 0.40285, testLoss: 0.58487, testAcc: 0.75\n",
      "Epoch: 5, TrLoss: 0.37669, testLoss: 0.54746, testAcc: 0.78\n",
      "Epoch: 6, TrLoss: 0.36592, testLoss: 0.61009, testAcc: 0.75\n",
      "Epoch: 7, TrLoss: 0.34329, testLoss: 0.53002, testAcc: 0.80\n",
      "Epoch: 8, TrLoss: 0.33527, testLoss: 0.55759, testAcc: 0.79\n",
      "Epoch: 9, TrLoss: 0.32160, testLoss: 0.54687, testAcc: 0.79\n",
      "Epoch: 10, TrLoss: 0.31466, testLoss: 0.62984, testAcc: 0.78\n",
      "Epoch: 11, TrLoss: 0.30135, testLoss: 0.54832, testAcc: 0.78\n",
      "Epoch: 12, TrLoss: 0.29476, testLoss: 0.57158, testAcc: 0.81\n",
      "Epoch: 13, TrLoss: 0.28930, testLoss: 0.57540, testAcc: 0.81\n",
      "Epoch: 14, TrLoss: 0.28231, testLoss: 0.52758, testAcc: 0.84\n",
      "Epoch: 15, TrLoss: 0.27499, testLoss: 0.60337, testAcc: 0.78\n",
      "Epoch: 16, TrLoss: 0.26959, testLoss: 0.54467, testAcc: 0.80\n",
      "Epoch: 17, TrLoss: 0.26839, testLoss: 0.54317, testAcc: 0.82\n",
      "Epoch: 18, TrLoss: 0.25967, testLoss: 0.64850, testAcc: 0.76\n",
      "Epoch: 19, TrLoss: 0.25530, testLoss: 0.57809, testAcc: 0.82\n",
      "Epoch: 20, TrLoss: 0.24078, testLoss: 0.57852, testAcc: 0.81\n",
      "Epoch: 21, TrLoss: 0.24041, testLoss: 0.60126, testAcc: 0.81\n",
      "Epoch: 22, TrLoss: 0.23709, testLoss: 0.56849, testAcc: 0.82\n",
      "Epoch: 23, TrLoss: 0.23512, testLoss: 0.59835, testAcc: 0.81\n",
      "Epoch: 24, TrLoss: 0.23328, testLoss: 0.62859, testAcc: 0.80\n",
      "Epoch: 25, TrLoss: 0.23310, testLoss: 0.55388, testAcc: 0.82\n",
      "Epoch: 26, TrLoss: 0.22503, testLoss: 0.54230, testAcc: 0.84\n",
      "Epoch: 27, TrLoss: 0.22727, testLoss: 0.61944, testAcc: 0.82\n",
      "Epoch: 28, TrLoss: 0.22430, testLoss: 0.53370, testAcc: 0.83\n",
      "Epoch: 29, TrLoss: 0.21812, testLoss: 0.60745, testAcc: 0.81\n",
      "Epoch: 30, TrLoss: 0.21393, testLoss: 0.63785, testAcc: 0.80\n",
      "Epoch: 31, TrLoss: 0.21309, testLoss: 0.63247, testAcc: 0.79\n",
      "Epoch: 32, TrLoss: 0.21208, testLoss: 0.60548, testAcc: 0.80\n",
      "Epoch: 33, TrLoss: 0.20908, testLoss: 0.60340, testAcc: 0.80\n",
      "Epoch: 34, TrLoss: 0.20523, testLoss: 0.63164, testAcc: 0.81\n",
      "Epoch: 35, TrLoss: 0.20443, testLoss: 0.56950, testAcc: 0.84\n",
      "Early stopping due to no improvement in validation accuracy\n",
      "Training finished\n",
      "Test AUC:  0.8840223125\n",
      "Saved model and results\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from copy import deepcopy\n",
    "\n",
    "def train(model, train_loader, val_loader, device, num_epochs=1000, batch_size=128,max_patience=20):\n",
    "    optimizer = optim.Adam(model.parameters(),weight_decay=0.001)\n",
    "    # optimizer = optim.Adam(model.parameters(),)\n",
    "\n",
    "    training_loss = []\n",
    "    test_losses = []\n",
    "    test_accs = []\n",
    "    \n",
    "    best_test_loss = 500000\n",
    "    best_weights = None\n",
    "    cur_patience = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        tr_loss = 0\n",
    "        for _, (data_cur, labels_cur) in enumerate(train_loader):\n",
    "            data_cur, labels_cur = data_cur.to(device), labels_cur.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred = model(data_cur).squeeze()  # Ensure y_pred is of the right shape\n",
    "            cur_loss = nn.functional.binary_cross_entropy(\n",
    "                    y_pred, labels_cur, reduction=\"mean\"\n",
    "                )  # Ensure labels_cur is float\n",
    "            # l2_lambda = 0.001\n",
    "            # l2_norm = sum(p.pow(2.0).sum() for p in model.features[-5].parameters())\n",
    "            # (cur_loss+l2_norm).backward()\n",
    "            cur_loss.backward()\n",
    "            tr_loss += cur_loss.item()                   \n",
    "            optimizer.step()\n",
    "\n",
    "        tr_loss /= len(train_loader)\n",
    "        training_loss.append(tr_loss)\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        test_acc = 0\n",
    "        with torch.no_grad():\n",
    "            for _, (data_cur, labels_cur) in enumerate(test_loader):\n",
    "                data_cur, labels_cur = data_cur.to(device), labels_cur.to(device)\n",
    "                y_pred = model(data_cur).squeeze()  # Ensure y_pred is of the right shape\n",
    "\n",
    "                cur_loss =nn.functional.binary_cross_entropy(y_pred, labels_cur , reduction=\"mean\")  # Ensure labels_cur is float\n",
    "                test_loss += cur_loss.item()\n",
    "\n",
    "                # Use the highest probability\n",
    "                y_pred_classes = y_pred.argmax(dim=1)  # Convert probabilities to class indices [0 or 1]\n",
    "                labels_classes = labels_cur.argmax(dim=1)  # Convert one-hot labels to class indices [0 or 1]\n",
    "                test_corr = (y_pred_classes == labels_classes).sum().item()\n",
    "                test_acc += test_corr\n",
    "\n",
    "        test_acc /= len(test_loader.dataset)\n",
    "        test_loss /= len(test_loader)\n",
    "\n",
    "        test_losses.append(test_loss)\n",
    "        test_accs.append(test_acc)\n",
    "\n",
    "        print(f\"Epoch: {epoch+1}, TrLoss: {tr_loss:.5f}, testLoss: {test_loss:.5f}, testAcc: {test_acc:.2f}\")\n",
    "\n",
    "        # Early stopping condition\n",
    "        if test_loss < best_test_loss:\n",
    "            best_weights = deepcopy(model.state_dict())\n",
    "            cur_patience = 0\n",
    "            best_test_loss = test_loss\n",
    "            \n",
    "        else:\n",
    "            cur_patience += 1\n",
    "\n",
    "        if cur_patience > max_patience:\n",
    "            print(\"Early stopping due to no improvement in validation accuracy\")\n",
    "            break\n",
    "    \n",
    "    print(\"Training finished\")\n",
    "\n",
    "    model.load_state_dict(best_weights)\n",
    "    np.random.seed()\n",
    "\n",
    "    file_name = \"\".join([str(np.random.choice(10)) for x in range(10)])\n",
    "\n",
    "    results = {}\n",
    "    results[\"filename\"] = file_name\n",
    "    for arg in vars(args):\n",
    "        if arg != \"save_path\":\n",
    "            results[str(arg)] = getattr(args, arg)\n",
    "    results[\"train_losses\"] = training_loss\n",
    "    results[\"test_losses\"] = test_losses\n",
    "    results[\"test_acc\"] = test_accs\n",
    "    results[\"best_test_loss\"] = best_test_loss\n",
    "    results[\"test_auc\"] = my_eval.calculate_roc_auc_score(\n",
    "        model, device, data[balanced_test_idxs], final_labels[balanced_test_idxs], batch_size\n",
    "    )\n",
    "    print(\"Test AUC: \", results[\"test_auc\"])\n",
    "\n",
    "    pkl.dump(results, open(os.path.join(save_path, file_name + \".pkl\"), \"wb\"))\n",
    "    torch.save(model.state_dict(), oj(save_path, file_name + \".pt\"))\n",
    "    print(\"Saved model and results\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train(model, train_loader, test_loader, device,\n",
    "        num_epochs=args.num_epochs,\n",
    "        max_patience=args.patience,\n",
    "        batch_size=args.batch_size,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6ea2cf5-8b43-4229-a450-6c7dc24dbd64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-20 01:26:48,411] A new study created in memory with name: no-name-5d0a7219-8318-4c68-b473-5169569ec622\n",
      "[I 2024-03-20 01:27:26,252] Trial 0 finished with value: 0.3628803137689829 and parameters: {'lr': 0.0009339960914009376, 'batch_size': 256, 'max_patience': 7}. Best is trial 0 with value: 0.3628803137689829.\n",
      "[I 2024-03-20 01:28:21,862] Trial 1 finished with value: 0.3921430818736553 and parameters: {'lr': 0.00012938388425188987, 'batch_size': 128, 'max_patience': 7}. Best is trial 0 with value: 0.3628803137689829.\n",
      "[I 2024-03-20 01:29:20,434] Trial 2 finished with value: 0.3860214464366436 and parameters: {'lr': 0.00012339558044188945, 'batch_size': 64, 'max_patience': 19}. Best is trial 0 with value: 0.3628803137689829.\n",
      "[I 2024-03-20 01:29:52,982] Trial 3 finished with value: 0.4097594106569886 and parameters: {'lr': 0.00012479148394647308, 'batch_size': 128, 'max_patience': 11}. Best is trial 0 with value: 0.3628803137689829.\n",
      "[I 2024-03-20 01:30:34,474] Trial 4 finished with value: 0.4219203582033515 and parameters: {'lr': 8.35127970792017e-05, 'batch_size': 128, 'max_patience': 13}. Best is trial 0 with value: 0.3628803137689829.\n",
      "[I 2024-03-20 01:31:16,714] Trial 5 finished with value: 0.40752782858908176 and parameters: {'lr': 0.00028556859991276463, 'batch_size': 64, 'max_patience': 10}. Best is trial 0 with value: 0.3628803137689829.\n",
      "[I 2024-03-20 01:31:51,236] Trial 6 finished with value: 0.3949250765144825 and parameters: {'lr': 0.00023297894590665406, 'batch_size': 256, 'max_patience': 11}. Best is trial 0 with value: 0.3628803137689829.\n",
      "[I 2024-03-20 01:33:37,097] Trial 7 finished with value: 0.3949266243726015 and parameters: {'lr': 0.0004025036511854602, 'batch_size': 64, 'max_patience': 19}. Best is trial 0 with value: 0.3628803137689829.\n",
      "[I 2024-03-20 01:35:03,892] Trial 8 finished with value: 0.36657372768968344 and parameters: {'lr': 0.0004962260646228508, 'batch_size': 64, 'max_patience': 14}. Best is trial 0 with value: 0.3628803137689829.\n",
      "[I 2024-03-20 01:36:06,891] Trial 9 finished with value: 0.4087277390062809 and parameters: {'lr': 7.753342211685305e-05, 'batch_size': 64, 'max_patience': 14}. Best is trial 0 with value: 0.3628803137689829.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "  Value (Best Validation Accuracy): 0.3628803137689829\n",
      "  Params: \n",
      "    lr: 0.0009339960914009376\n",
      "    batch_size: 256\n",
      "    max_patience: 7\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    # Suggest hyperparameters\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-3, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [64, 128, 256])\n",
    "    num_epochs = 100  # Reduced for faster trials\n",
    "    max_patience = trial.suggest_int('max_patience', 5, 20)\n",
    "\n",
    "    # Initialize your model (adjust as necessary)\n",
    "    \n",
    "    # Use the suggested learning rate\n",
    "    optimizer = Adam(model.parameters(), lr=lr, weight_decay=0.001)\n",
    "\n",
    "\n",
    "    best_val_loss = 50000\n",
    "    cur_patience = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        tr_loss = 0\n",
    "        for _, (data_cur, labels_cur) in enumerate(train_loader):\n",
    "            data_cur, labels_cur = data_cur.to(device), labels_cur.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred = model(data_cur).squeeze()\n",
    "            cur_loss = nn.functional.binary_cross_entropy(y_pred, labels_cur.float(), reduction=\"mean\")\n",
    "            cur_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            tr_loss += cur_loss.item()\n",
    "\n",
    "        tr_loss /= len(train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_acc = 0\n",
    "        with torch.no_grad():\n",
    "            for _, (data_cur, labels_cur) in enumerate(val_loader):\n",
    "                data_cur, labels_cur = data_cur.to(device), labels_cur.to(device)\n",
    "                y_pred = model(data_cur).squeeze()\n",
    "\n",
    "                cur_loss = nn.functional.binary_cross_entropy(y_pred, labels_cur.float(), reduction=\"mean\")\n",
    "                val_loss += cur_loss.item()\n",
    "\n",
    "                # predicted = (torch.sigmoid(y_pred) > 0.5).long()\n",
    "                # val_corr = (predicted == labels_cur).sum()\n",
    "                # val_acc += val_corr.item()\n",
    "                y_pred_classes = y_pred.argmax(dim=1)  # Convert probabilities to class indices [0 or 1]\n",
    "                labels_classes = labels_cur.argmax(dim=1)  # Convert one-hot labels to class indices [0 or 1]\n",
    "                val_corr = (y_pred_classes == labels_classes).sum().item()\n",
    "                val_acc += val_corr\n",
    "\n",
    "        val_acc /= len(val_loader.dataset)\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        # Report the validation loss to optuna\n",
    "        trial.report(val_loss, epoch)\n",
    "\n",
    "        # Early stopping condition\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            cur_patience = 0\n",
    "        else:\n",
    "            cur_patience += 1\n",
    "            if cur_patience > max_patience:\n",
    "                break\n",
    "\n",
    "\n",
    "    return best_val_loss\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=10)  # Adjust the number of trials\n",
    "\n",
    "# Best trial result\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\"  Value (Best Validation Accuracy): {trial.value}\")\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d18e72f7-2004-468c-80e7-2f477618d3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optuna\n",
      "  Obtaining dependency information for optuna from https://files.pythonhosted.org/packages/4c/6a/219a431aaf81b3eb3070fd2d58116baa366d3072f43bbcc87dc3495b7546/optuna-3.5.0-py3-none-any.whl.metadata\n",
      "  Downloading optuna-3.5.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting alembic>=1.5.0 (from optuna)\n",
      "  Obtaining dependency information for alembic>=1.5.0 from https://files.pythonhosted.org/packages/7f/50/9fb3a5c80df6eb6516693270621676980acd6d5a9a7efdbfa273f8d616c7/alembic-1.13.1-py3-none-any.whl.metadata\n",
      "  Downloading alembic-1.13.1-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting colorlog (from optuna)\n",
      "  Obtaining dependency information for colorlog from https://files.pythonhosted.org/packages/f3/18/3e867ab37a24fdf073c1617b9c7830e06ec270b1ea4694a624038fc40a03/colorlog-6.8.2-py3-none-any.whl.metadata\n",
      "  Downloading colorlog-6.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\aethe\\anaconda3\\lib\\site-packages (from optuna) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\aethe\\anaconda3\\lib\\site-packages (from optuna) (23.1)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in c:\\users\\aethe\\anaconda3\\lib\\site-packages (from optuna) (2.0.25)\n",
      "Requirement already satisfied: tqdm in c:\\users\\aethe\\anaconda3\\lib\\site-packages (from optuna) (4.65.0)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\aethe\\anaconda3\\lib\\site-packages (from optuna) (6.0.1)\n",
      "Collecting Mako (from alembic>=1.5.0->optuna)\n",
      "  Obtaining dependency information for Mako from https://files.pythonhosted.org/packages/2b/8d/9f11d0b9ac521febb806e7f30dc5982d0f4f5821217712c59005fbc5c1e3/Mako-1.3.2-py3-none-any.whl.metadata\n",
      "  Downloading Mako-1.3.2-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4 in c:\\users\\aethe\\anaconda3\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.7.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\aethe\\anaconda3\\lib\\site-packages (from sqlalchemy>=1.3.0->optuna) (3.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\aethe\\anaconda3\\lib\\site-packages (from colorlog->optuna) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\aethe\\anaconda3\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n",
      "Downloading optuna-3.5.0-py3-none-any.whl (413 kB)\n",
      "   ---------------------------------------- 0.0/413.4 kB ? eta -:--:--\n",
      "   ------------- -------------------------- 143.4/413.4 kB 4.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 413.4/413.4 kB 6.5 MB/s eta 0:00:00\n",
      "Downloading alembic-1.13.1-py3-none-any.whl (233 kB)\n",
      "   ---------------------------------------- 0.0/233.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 233.4/233.4 kB 14.9 MB/s eta 0:00:00\n",
      "Downloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
      "Downloading Mako-1.3.2-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.7/78.7 kB ? eta 0:00:00\n",
      "Installing collected packages: Mako, colorlog, alembic, optuna\n",
      "Successfully installed Mako-1.3.2 alembic-1.13.1 colorlog-6.8.2 optuna-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "753999dd-9914-4890-91d2-54a0f89fc33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1d output shape: torch.Size([1, 4, 247])\n",
      "ReLU output shape: torch.Size([1, 4, 247])\n",
      "Dropout output shape: torch.Size([1, 4, 247])\n",
      "BatchNorm1d output shape: torch.Size([1, 4, 247])\n",
      "Conv1d output shape: torch.Size([1, 4, 245])\n",
      "ReLU output shape: torch.Size([1, 4, 245])\n",
      "Dropout output shape: torch.Size([1, 4, 245])\n",
      "BatchNorm1d output shape: torch.Size([1, 4, 245])\n",
      "MaxPool1d output shape: torch.Size([1, 4, 122])\n",
      "Conv1d output shape: torch.Size([1, 8, 120])\n",
      "ReLU output shape: torch.Size([1, 8, 120])\n",
      "Dropout output shape: torch.Size([1, 8, 120])\n",
      "BatchNorm1d output shape: torch.Size([1, 8, 120])\n",
      "Conv1d output shape: torch.Size([1, 8, 118])\n",
      "ReLU output shape: torch.Size([1, 8, 118])\n",
      "Dropout output shape: torch.Size([1, 8, 118])\n",
      "MaxPool1d output shape: torch.Size([1, 8, 59])\n",
      "BatchNorm1d output shape: torch.Size([1, 8, 59])\n",
      "Conv1d output shape: torch.Size([1, 8, 57])\n",
      "ReLU output shape: torch.Size([1, 8, 57])\n",
      "Dropout output shape: torch.Size([1, 8, 57])\n",
      "BatchNorm1d output shape: torch.Size([1, 8, 57])\n",
      "Flatten output shape: torch.Size([1, 456])\n",
      "456\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = models.FGANet(\n",
    "    num_input=data.shape[2],\n",
    "    num_output=1,\n",
    "    conv_channels=4,\n",
    "    num_in_channels=data.shape[1])\n",
    "model.eval()\n",
    "# Create a dummy input with the size of [batch_size, channels, width]\n",
    "# where channels is num_in_channels and width is 249\n",
    "dummy_input = torch.randn(1, data.shape[1], 249)\n",
    "\n",
    "with torch.no_grad():\n",
    "    x = dummy_input\n",
    "    for layer in model.features:\n",
    "        x = layer(x)\n",
    "        print(layer.__class__.__name__, \"output shape:\", x.shape)\n",
    "\n",
    "# Flatten the output from the convolutional layers\n",
    "# The view operation reshapes the output to a two-dimensional tensor where the second dimension\n",
    "# is the total number of features from the convolutional output\n",
    "num_dense_input = conv_output.view(conv_output.size(0), -1).size(1)\n",
    "\n",
    "# Now num_dense_input holds the total number of features for the fully connected layer\n",
    "print(num_dense_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29b1a6f7-1e4e-43e7-9a2f-cd349a4a5ff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "249"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be93d055-fedf-420b-976c-c844e9073eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1d output shape: torch.Size([1, 16, 247])\n",
      "ReLU output shape: torch.Size([1, 16, 247])\n",
      "Dropout output shape: torch.Size([1, 16, 247])\n",
      "BatchNorm1d output shape: torch.Size([1, 16, 247])\n",
      "Conv1d output shape: torch.Size([1, 16, 245])\n",
      "ReLU output shape: torch.Size([1, 16, 245])\n",
      "Dropout output shape: torch.Size([1, 16, 245])\n",
      "BatchNorm1d output shape: torch.Size([1, 16, 245])\n",
      "MaxPool1d output shape: torch.Size([1, 16, 122])\n",
      "Conv1d output shape: torch.Size([1, 32, 120])\n",
      "ReLU output shape: torch.Size([1, 32, 120])\n",
      "Dropout output shape: torch.Size([1, 32, 120])\n",
      "BatchNorm1d output shape: torch.Size([1, 32, 120])\n",
      "Conv1d output shape: torch.Size([1, 32, 118])\n",
      "ReLU output shape: torch.Size([1, 32, 118])\n",
      "Dropout output shape: torch.Size([1, 32, 118])\n",
      "MaxPool1d output shape: torch.Size([1, 32, 59])\n",
      "BatchNorm1d output shape: torch.Size([1, 32, 59])\n",
      "Flatten output shape: torch.Size([1, 1888])\n"
     ]
    }
   ],
   "source": [
    "model = models.FGANet(\n",
    "    num_input=data.shape[2],\n",
    "    num_output=2,\n",
    "    conv_channels=args.num_conv,\n",
    "    num_in_channels=data.shape[1],\n",
    "    stride=1,)\n",
    "model.eval()\n",
    "dummy_input = torch.randn(1, data.shape[1], 249)\n",
    "with torch.no_grad():\n",
    "    x = dummy_input\n",
    "    for layer in model.features:\n",
    "        x = layer(x)\n",
    "        print(layer.__class__.__name__, \"output shape:\", x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71c8fc76-54a6-4e7a-ae8c-c41710a0da37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.num_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab4db110-38c7-430f-8943-add35d97c3f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_weights' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(best_weights)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'best_weights' is not defined"
     ]
    }
   ],
   "source": [
    "print(best_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08804575-6bae-4e21-a7ef-89ea241ef63a",
   "metadata": {},
   "source": [
    "model.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b655b546-0876-4d79-99c0-268f5c3a79a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv1d(1, 16, kernel_size=(3,), stride=(1,))\n",
       "  (1): ReLU()\n",
       "  (2): Dropout(p=0.2, inplace=False)\n",
       "  (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (4): Conv1d(16, 16, kernel_size=(3,), stride=(1,))\n",
       "  (5): ReLU()\n",
       "  (6): Dropout(p=0.3, inplace=False)\n",
       "  (7): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (8): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (9): Conv1d(16, 32, kernel_size=(3,), stride=(1,))\n",
       "  (10): ReLU()\n",
       "  (11): Dropout(p=0.3, inplace=False)\n",
       "  (12): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (13): Conv1d(32, 32, kernel_size=(3,), stride=(1,))\n",
       "  (14): ReLU()\n",
       "  (15): Dropout(p=0.4, inplace=False)\n",
       "  (16): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (17): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (18): Conv1d(32, 32, kernel_size=(3,), stride=(1,))\n",
       "  (19): ReLU()\n",
       "  (20): Dropout(p=0.5, inplace=False)\n",
       "  (21): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (22): Flatten(start_dim=1, end_dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77811197-6a54-45fc-b0ac-2728c970f978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv1d(32, 32, kernel_size=(3,), stride=(1,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.features[-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbe1493e-c4d7-40a3-892a-90c596580036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32954, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38db386-e7ae-4450-9e06-3840d1c3993b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
