{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af0b8c01-a470-4e41-b2bd-974a75c74f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "(32954, 249)\n",
      "[14 18 17 32 13 33 20  8 36 28 29 10 44  3 22 48 30 35 24 23  4 42  7  1\n",
      " 49 41 45 15 46 16 34 37  0  5 21]\n",
      "[11  2 43  6 25 26 39]\n",
      "[27 12 31 47  9 40 38 19]\n",
      "(23208,)\n",
      "(5182,)\n",
      "(4564,)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle as pkl\n",
    "from argparse import ArgumentParser\n",
    "from copy import deepcopy\n",
    "from os.path import join as oj\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import configparser\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.utils.data\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import csv\n",
    "import optuna\n",
    "\n",
    "sys.path.insert(0, \"../1D CNN\")\n",
    "import utils\n",
    "import models\n",
    "import data_fns\n",
    "import my_eval\n",
    "\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed()\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"../config.ini\")\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def get_args():\n",
    "    parser = ArgumentParser(description=\"Functional group analysis\")\n",
    "\n",
    "    parser.add_argument(\"--exp_name\", type=str, default=\"\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=128)\n",
    "    parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
    "    parser.add_argument(\"--seed\", type=int, default=0)\n",
    "\n",
    "    parser.add_argument(\"--num_conv\", type=int, default=16)\n",
    "    parser.add_argument(\"--patience\", type=int, default=5)\n",
    "    ret_args, unknown = parser.parse_known_args()\n",
    "    return ret_args\n",
    "\n",
    "\n",
    "args = get_args()\n",
    "\n",
    "\n",
    "# data = np.load(\"data.npy\")\n",
    "# labels = np.load('labels.npy')\n",
    "\n",
    "# filters = ((\"state\", \"gas\"), (\"yunits\", \"ABSORBANCE\"), (\"xunits\", \"1/CM\"))\n",
    "# data, labels = utils.fixed_domain_filter(\n",
    "#     irdata, irindex, filters\n",
    "# )  # Filter points from the dataset\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "\n",
    "file_path = \"TS_freq_array_0208.csv\"\n",
    "save_path = \"C:\\\\Users\\\\aethe\\\\1D CNN\"\n",
    "# Initialize arrays\n",
    "y_labels = []\n",
    "x_TS = []\n",
    "fish_num = []\n",
    "\n",
    "# Read the CSV file line by line\n",
    "with open(file_path, 'r') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    \n",
    "    next(csv_reader)\n",
    "    \n",
    "    # Extract the first column and construct rows as arrays\n",
    "    for row in csv_reader:\n",
    "        fish_num.append(row[0])\n",
    "        y_labels.append(row[1])\n",
    "        x_TS.append(row[2:])\n",
    "\n",
    "\n",
    "x_TS = np.array(x_TS, dtype = float)\n",
    "\n",
    "print(x_TS.shape) # (32954, 249)\n",
    "\n",
    "means = np.mean(x_TS, axis=0)\n",
    "std_devs = np.std(x_TS, axis=0)\n",
    "x_TS = (x_TS - means) / std_devs  # Standardizing data\n",
    "\n",
    "\n",
    "# Contruct data and labels\n",
    "data = x_TS[:, None]\n",
    "labels = np.array(y_labels, dtype=int)\n",
    "\n",
    "\n",
    "# When spliting the dataset, we first split based on each fish to aviod data leakage\n",
    "# Create a mapping dictionary from fish_num to indices\n",
    "\n",
    "fish_num_to_index = {fish_num: index for index, fish_num in enumerate(np.unique(np.array(fish_num)))}\n",
    "\n",
    "# Use the mapping to convert the fish_num to indices\n",
    "fish_ind = np.array([fish_num_to_index[num] for num in fish_num])\n",
    "\n",
    "\n",
    "train_idxs_by_fish, val_idxs_by_fish, test_idxs_by_fish = data_fns.get_split(len(np.unique(fish_ind)), seed=23)\n",
    "\n",
    "# Checn index split by fish\n",
    "print(train_idxs_by_fish)\n",
    "print(val_idxs_by_fish)\n",
    "print(test_idxs_by_fish)\n",
    "\n",
    "# This function takes in the indices that split by individual fish\n",
    "# It then returns the indices split by each time ping correspond to the individual fish\n",
    "def construct_data_index_by_fish_idxs(idxs_split_by_fish, fish_ind):\n",
    "    indx = np.array([], dtype=int)  # Ensure the array is of integer type\n",
    "    for split_fish_ind in idxs_split_by_fish:\n",
    "        # Ensure split_fish_ind is an integer before using it as an index\n",
    "        split_fish_ind = int(split_fish_ind)\n",
    "        indx_current_fish = np.where(fish_ind == split_fish_ind)[0]\n",
    "        indx = np.append(indx, indx_current_fish)\n",
    "\n",
    "    return indx\n",
    "\n",
    "\n",
    "\n",
    "# Indices that we use in the model\n",
    "train_idxs = construct_data_index_by_fish_idxs(train_idxs_by_fish, fish_ind)\n",
    "val_idxs = construct_data_index_by_fish_idxs(val_idxs_by_fish, fish_ind)\n",
    "test_idxs = construct_data_index_by_fish_idxs(test_idxs_by_fish, fish_ind)\n",
    "\n",
    "\n",
    "\n",
    "print(train_idxs.shape) # (22925,)\n",
    "print(val_idxs.shape) # (4662,)\n",
    "print(test_idxs.shape) # (5367,)\n",
    "\n",
    "def balance_classes_by_indices(indices, labels, n_samples_per_class):\n",
    "    unique_classes = np.unique(labels[indices])\n",
    "    balanced_indices = np.array([], dtype=int)\n",
    "\n",
    "    for cls in unique_classes:\n",
    "        class_indices = indices[labels[indices] == cls]\n",
    "        if len(class_indices) > n_samples_per_class:\n",
    "            class_indices = np.random.choice(class_indices, n_samples_per_class, replace=False)\n",
    "        else:\n",
    "            class_indices = np.random.choice(class_indices, n_samples_per_class, replace=True)\n",
    "        balanced_indices = np.append(balanced_indices, class_indices)\n",
    "\n",
    "    np.random.shuffle(balanced_indices)  # Shuffle to mix classes\n",
    "    return balanced_indices\n",
    "\n",
    "# Apply balancing to each split\n",
    "balanced_train_idxs = balance_classes_by_indices(train_idxs, labels, n_samples_per_class=10029)\n",
    "balanced_val_idxs = balance_classes_by_indices(val_idxs, labels, n_samples_per_class=1000)\n",
    "balanced_test_idxs = balance_classes_by_indices(test_idxs, labels, n_samples_per_class=2000)\n",
    "\n",
    "\n",
    "#%%\n",
    "# num_classes = labels.shape[1]\n",
    "num_classes = 2\n",
    "# train_idxs, val_idxs, test_idxs = data_fns.get_split(len(data), seed=42)\n",
    "\n",
    "# scale data\n",
    "# weights = labels[train_idxs].mean(axis=0)\n",
    "# anti_weights = 1 - weights\n",
    "\n",
    "# mult_weights = weights * anti_weights\n",
    "# weights /= mult_weights\n",
    "# anti_weights /= mult_weights\n",
    "\n",
    "# weights = torch.tensor(weights).to(device)\n",
    "# anti_weights = torch.tensor(anti_weights).to(device)\n",
    "\n",
    "\n",
    "#%%\n",
    "# create datasets in torch\n",
    "torch.manual_seed(args.seed)\n",
    "train_dataset = TensorDataset(\n",
    "    *[torch.Tensor(input) for input in [data[balanced_train_idxs], labels[balanced_train_idxs]]],\n",
    ")\n",
    "val_dataset = TensorDataset(\n",
    "    *[torch.Tensor(input) for input in [data[balanced_val_idxs], labels[balanced_val_idxs]]],\n",
    ")\n",
    "\n",
    "test_dataset = TensorDataset(\n",
    "    *[torch.Tensor(input) for input in [data[balanced_test_idxs], labels[balanced_test_idxs]]],\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "print(num_classes)\n",
    "\n",
    "\n",
    "\n",
    "model = models.FGANet(\n",
    "    num_input=data.shape[2],\n",
    "    num_output=1,\n",
    "    conv_channels=args.num_conv,\n",
    "    num_in_channels=data.shape[1],\n",
    "    stride=1,\n",
    ").to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2477c9b-ff17-49d7-9231-b76a558679f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, TrLoss: 0.61802, testLoss: 0.61843, testAcc: 0.65\n",
      "Epoch: 2, TrLoss: 0.53314, testLoss: 0.53229, testAcc: 0.74\n",
      "Epoch: 3, TrLoss: 0.47550, testLoss: 0.48410, testAcc: 0.80\n",
      "Epoch: 4, TrLoss: 0.44049, testLoss: 0.47578, testAcc: 0.82\n",
      "Epoch: 5, TrLoss: 0.40831, testLoss: 0.43979, testAcc: 0.84\n",
      "Epoch: 6, TrLoss: 0.38507, testLoss: 0.47160, testAcc: 0.84\n",
      "Epoch: 7, TrLoss: 0.36242, testLoss: 0.45455, testAcc: 0.83\n",
      "Epoch: 8, TrLoss: 0.33976, testLoss: 0.51618, testAcc: 0.83\n",
      "Epoch: 9, TrLoss: 0.32060, testLoss: 0.53170, testAcc: 0.84\n",
      "Epoch: 10, TrLoss: 0.30759, testLoss: 0.54150, testAcc: 0.81\n",
      "Epoch: 11, TrLoss: 0.28730, testLoss: 0.50364, testAcc: 0.85\n",
      "Epoch: 12, TrLoss: 0.27618, testLoss: 0.57406, testAcc: 0.85\n",
      "Epoch: 13, TrLoss: 0.25931, testLoss: 0.54036, testAcc: 0.85\n",
      "Epoch: 14, TrLoss: 0.25168, testLoss: 0.58073, testAcc: 0.85\n",
      "Epoch: 15, TrLoss: 0.24423, testLoss: 0.60522, testAcc: 0.85\n",
      "Epoch: 16, TrLoss: 0.23312, testLoss: 0.58519, testAcc: 0.84\n",
      "Epoch: 17, TrLoss: 0.22580, testLoss: 0.58664, testAcc: 0.85\n",
      "Epoch: 18, TrLoss: 0.21838, testLoss: 0.54428, testAcc: 0.86\n",
      "Epoch: 19, TrLoss: 0.20868, testLoss: 0.62982, testAcc: 0.84\n",
      "Epoch: 20, TrLoss: 0.20130, testLoss: 0.55943, testAcc: 0.84\n",
      "Epoch: 21, TrLoss: 0.18926, testLoss: 0.65657, testAcc: 0.83\n",
      "Epoch: 22, TrLoss: 0.19129, testLoss: 0.55105, testAcc: 0.85\n",
      "Epoch: 23, TrLoss: 0.18040, testLoss: 0.63200, testAcc: 0.85\n",
      "Epoch: 24, TrLoss: 0.17342, testLoss: 0.59536, testAcc: 0.84\n",
      "Early stopping due to no improvement in validation accuracy\n",
      "Training finished\n",
      "Test AUC:  0.8989785\n",
      "Saved model and results\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from copy import deepcopy\n",
    "\n",
    "def train(model, train_loader, val_loader, device, num_epochs=1000, batch_size=128,max_patience=20):\n",
    "    optimizer = optim.Adam(model.parameters(),)\n",
    "\n",
    "    training_loss = []\n",
    "    test_losses = []\n",
    "    test_accs = []\n",
    "    \n",
    "    best_test_acc = 0\n",
    "    best_weights = None\n",
    "    cur_patience = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        tr_loss = 0\n",
    "        for _, (data_cur, labels_cur) in enumerate(train_loader):\n",
    "            data_cur, labels_cur = data_cur.to(device), labels_cur.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred = model(data_cur).squeeze()  # Ensure y_pred is of the right shape\n",
    "            cur_loss = nn.functional.binary_cross_entropy(\n",
    "                    y_pred, labels_cur, reduction=\"mean\"\n",
    "                )  # Ensure labels_cur is float\n",
    "            l2_lambda = 0.001\n",
    "            l2_norm = sum(p.pow(2.0).sum() for p in model.features[-9].parameters())\n",
    "            (cur_loss+l2_norm).backward()\n",
    "            tr_loss += cur_loss.item()                   \n",
    "            optimizer.step()\n",
    "\n",
    "        tr_loss /= len(train_loader)\n",
    "        training_loss.append(tr_loss)\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        test_acc = 0\n",
    "        with torch.no_grad():\n",
    "            for _, (data_cur, labels_cur) in enumerate(test_loader):\n",
    "                data_cur, labels_cur = data_cur.to(device), labels_cur.to(device)\n",
    "                y_pred = model(data_cur).squeeze()  # Ensure y_pred is of the right shape\n",
    "\n",
    "                cur_loss =nn.functional.binary_cross_entropy(y_pred, labels_cur , reduction=\"mean\")  # Ensure labels_cur is float\n",
    "                test_loss += cur_loss.item()\n",
    "\n",
    "                # For binary classification, use a threshold (e.g., 0.5) to determine the predicted class\n",
    "                test_corr =  ((y_pred > 0.5) == labels_cur).sum()\n",
    "                test_acc += test_corr.item()\n",
    "\n",
    "        test_acc /= len(test_loader.dataset)\n",
    "        test_loss /= len(test_loader)\n",
    "\n",
    "        test_losses.append(test_loss)\n",
    "        test_accs.append(test_acc)\n",
    "\n",
    "        print(f\"Epoch: {epoch+1}, TrLoss: {tr_loss:.5f}, testLoss: {test_loss:.5f}, testAcc: {test_acc:.2f}\")\n",
    "\n",
    "        # Early stopping condition\n",
    "        if test_acc > best_test_acc:\n",
    "            best_weights = deepcopy(model.state_dict())\n",
    "            cur_patience = 0\n",
    "            best_test_acc = test_acc\n",
    "        else:\n",
    "            cur_patience += 1\n",
    "\n",
    "        if cur_patience > max_patience:\n",
    "            print(\"Early stopping due to no improvement in validation accuracy\")\n",
    "            break\n",
    "    \n",
    "    print(\"Training finished\")\n",
    "\n",
    "    model.load_state_dict(best_weights)\n",
    "    np.random.seed()\n",
    "\n",
    "    file_name = \"\".join([str(np.random.choice(10)) for x in range(10)])\n",
    "\n",
    "    results = {}\n",
    "    results[\"filename\"] = file_name\n",
    "    for arg in vars(args):\n",
    "        if arg != \"save_path\":\n",
    "            results[str(arg)] = getattr(args, arg)\n",
    "    results[\"train_losses\"] = training_loss\n",
    "    results[\"test_acc\"] = test_accs\n",
    "    results[\"best_test_acc\"] = best_test_acc\n",
    "    results[\"test_auc\"] = my_eval.calculate_roc_auc_score(\n",
    "        model, device, data[balanced_test_idxs], labels[balanced_test_idxs], batch_size\n",
    "    )\n",
    "    print(\"Test AUC: \", results[\"test_auc\"])\n",
    "\n",
    "    pkl.dump(results, open(os.path.join(save_path, file_name + \".pkl\"), \"wb\"))\n",
    "    torch.save(model.state_dict(), oj(save_path, file_name + \".pt\"))\n",
    "    print(\"Saved model and results\")\n",
    "\n",
    "    return results\n",
    "\n",
    "def get_prec_recall_f1(model, x_used, y_used, thresholds):\n",
    "    best_f1 = np.zeros((y_used.shape[1]))\n",
    "    best_precision = np.zeros((y_used.shape[1]))\n",
    "    best_recall = np.zeros((y_used.shape[1]))\n",
    "    best_accuracy = np.zeros((y_used.shape[1]))\n",
    "    pred_used = model.forward(torch.Tensor(x_used).cuda()).detach().cpu().numpy() > thresholds[None, :]\n",
    "\n",
    "\n",
    "    for i in range(17):\n",
    "        best_precision[i] = (pred_used[:,i] * y_used[:,i])[np.where(pred_used[:,i])].mean()\n",
    "        best_recall[i] = (pred_used[:,i] * y_used[:,i])[np.where(y_used[:,i])].mean()\n",
    "        best_f1[i] = 2*best_recall[i]*best_precision[i] /(best_recall[i]+best_precision[i])\n",
    "        best_accuracy[i] = (pred_used[:,i] == y_used[:,i]).mean()\n",
    "    return best_precision, best_recall, best_f1, best_accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train(model, train_loader, test_loader, device,\n",
    "        num_epochs=args.num_epochs,\n",
    "        max_patience=args.patience,\n",
    "        batch_size=args.batch_size,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d6ea2cf5-8b43-4229-a450-6c7dc24dbd64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-15 02:02:39,980] A new study created in memory with name: no-name-5cb244e7-463f-4748-aca1-8c86eded4598\n",
      "[I 2024-02-15 02:03:46,138] Trial 0 finished with value: 0.709 and parameters: {'lr': 3.387791467999433e-05, 'batch_size': 256, 'max_patience': 17}. Best is trial 0 with value: 0.709.\n",
      "[I 2024-02-15 02:04:19,522] Trial 1 finished with value: 0.728 and parameters: {'lr': 0.0001326440079283532, 'batch_size': 64, 'max_patience': 15}. Best is trial 1 with value: 0.728.\n",
      "[I 2024-02-15 02:05:15,367] Trial 2 finished with value: 0.7315 and parameters: {'lr': 1.3329316003833078e-05, 'batch_size': 64, 'max_patience': 9}. Best is trial 2 with value: 0.7315.\n",
      "[I 2024-02-15 02:05:30,667] Trial 3 finished with value: 0.7625 and parameters: {'lr': 0.0007950516792503219, 'batch_size': 256, 'max_patience': 9}. Best is trial 3 with value: 0.7625.\n",
      "[I 2024-02-15 02:05:47,149] Trial 4 finished with value: 0.7555 and parameters: {'lr': 7.779185194440822e-05, 'batch_size': 256, 'max_patience': 13}. Best is trial 3 with value: 0.7625.\n",
      "[I 2024-02-15 02:07:30,368] Trial 5 finished with value: 0.77275 and parameters: {'lr': 6.638451758271618e-05, 'batch_size': 64, 'max_patience': 12}. Best is trial 5 with value: 0.77275.\n",
      "[I 2024-02-15 02:08:30,653] Trial 6 finished with value: 0.78125 and parameters: {'lr': 0.00016854472215909968, 'batch_size': 128, 'max_patience': 14}. Best is trial 6 with value: 0.78125.\n",
      "[I 2024-02-15 02:09:27,639] Trial 7 finished with value: 0.7845 and parameters: {'lr': 2.415661552412808e-05, 'batch_size': 64, 'max_patience': 15}. Best is trial 7 with value: 0.7845.\n",
      "[I 2024-02-15 02:09:42,036] Trial 8 finished with value: 0.784 and parameters: {'lr': 2.3792215964303348e-05, 'batch_size': 128, 'max_patience': 9}. Best is trial 7 with value: 0.7845.\n",
      "[I 2024-02-15 02:10:06,674] Trial 9 finished with value: 0.77625 and parameters: {'lr': 0.0006188847716585719, 'batch_size': 128, 'max_patience': 19}. Best is trial 7 with value: 0.7845.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "  Value (Best Validation Accuracy): 0.7845\n",
      "  Params: \n",
      "    lr: 2.415661552412808e-05\n",
      "    batch_size: 64\n",
      "    max_patience: 15\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    # Suggest hyperparameters\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-3, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [64, 128, 256])\n",
    "    num_epochs = 100  # Reduced for faster trials\n",
    "    max_patience = trial.suggest_int('max_patience', 5, 20)\n",
    "\n",
    "    # Initialize your model (adjust as necessary)\n",
    "    \n",
    "    # Use the suggested learning rate\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "    best_val_acc = 0\n",
    "    cur_patience = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        tr_loss = 0\n",
    "        for _, (data_cur, labels_cur) in enumerate(train_loader):\n",
    "            data_cur, labels_cur = data_cur.to(device), labels_cur.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred = model(data_cur).squeeze()\n",
    "            cur_loss = nn.functional.binary_cross_entropy(y_pred, labels_cur.float(), reduction=\"mean\")\n",
    "            cur_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            tr_loss += cur_loss.item()\n",
    "\n",
    "        tr_loss /= len(train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_acc = 0\n",
    "        with torch.no_grad():\n",
    "            for _, (data_cur, labels_cur) in enumerate(val_loader):\n",
    "                data_cur, labels_cur = data_cur.to(device), labels_cur.to(device)\n",
    "                y_pred = model(data_cur).squeeze()\n",
    "\n",
    "                cur_loss = nn.functional.binary_cross_entropy(y_pred, labels_cur.float(), reduction=\"mean\")\n",
    "                val_loss += cur_loss.item()\n",
    "\n",
    "                predicted = (torch.sigmoid(y_pred) > 0.5).long()\n",
    "                val_corr = (predicted == labels_cur).sum()\n",
    "                val_acc += val_corr.item()\n",
    "\n",
    "        val_acc /= len(val_loader.dataset)\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        # Report the validation loss to optuna\n",
    "        trial.report(val_loss, epoch)\n",
    "\n",
    "        # Early stopping condition\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            cur_patience = 0\n",
    "        else:\n",
    "            cur_patience += 1\n",
    "            if cur_patience > max_patience:\n",
    "                break\n",
    "\n",
    "\n",
    "    return best_val_acc\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=10)  # Adjust the number of trials\n",
    "\n",
    "# Best trial result\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\"  Value (Best Validation Accuracy): {trial.value}\")\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d18e72f7-2004-468c-80e7-2f477618d3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optuna\n",
      "  Obtaining dependency information for optuna from https://files.pythonhosted.org/packages/4c/6a/219a431aaf81b3eb3070fd2d58116baa366d3072f43bbcc87dc3495b7546/optuna-3.5.0-py3-none-any.whl.metadata\n",
      "  Downloading optuna-3.5.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting alembic>=1.5.0 (from optuna)\n",
      "  Obtaining dependency information for alembic>=1.5.0 from https://files.pythonhosted.org/packages/7f/50/9fb3a5c80df6eb6516693270621676980acd6d5a9a7efdbfa273f8d616c7/alembic-1.13.1-py3-none-any.whl.metadata\n",
      "  Downloading alembic-1.13.1-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting colorlog (from optuna)\n",
      "  Obtaining dependency information for colorlog from https://files.pythonhosted.org/packages/f3/18/3e867ab37a24fdf073c1617b9c7830e06ec270b1ea4694a624038fc40a03/colorlog-6.8.2-py3-none-any.whl.metadata\n",
      "  Downloading colorlog-6.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\aethe\\anaconda3\\lib\\site-packages (from optuna) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\aethe\\anaconda3\\lib\\site-packages (from optuna) (23.1)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in c:\\users\\aethe\\anaconda3\\lib\\site-packages (from optuna) (2.0.25)\n",
      "Requirement already satisfied: tqdm in c:\\users\\aethe\\anaconda3\\lib\\site-packages (from optuna) (4.65.0)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\aethe\\anaconda3\\lib\\site-packages (from optuna) (6.0.1)\n",
      "Collecting Mako (from alembic>=1.5.0->optuna)\n",
      "  Obtaining dependency information for Mako from https://files.pythonhosted.org/packages/2b/8d/9f11d0b9ac521febb806e7f30dc5982d0f4f5821217712c59005fbc5c1e3/Mako-1.3.2-py3-none-any.whl.metadata\n",
      "  Downloading Mako-1.3.2-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4 in c:\\users\\aethe\\anaconda3\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.7.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\aethe\\anaconda3\\lib\\site-packages (from sqlalchemy>=1.3.0->optuna) (3.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\aethe\\anaconda3\\lib\\site-packages (from colorlog->optuna) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\aethe\\anaconda3\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n",
      "Downloading optuna-3.5.0-py3-none-any.whl (413 kB)\n",
      "   ---------------------------------------- 0.0/413.4 kB ? eta -:--:--\n",
      "   ------------- -------------------------- 143.4/413.4 kB 4.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 413.4/413.4 kB 6.5 MB/s eta 0:00:00\n",
      "Downloading alembic-1.13.1-py3-none-any.whl (233 kB)\n",
      "   ---------------------------------------- 0.0/233.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 233.4/233.4 kB 14.9 MB/s eta 0:00:00\n",
      "Downloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
      "Downloading Mako-1.3.2-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.7/78.7 kB ? eta 0:00:00\n",
      "Installing collected packages: Mako, colorlog, alembic, optuna\n",
      "Successfully installed Mako-1.3.2 alembic-1.13.1 colorlog-6.8.2 optuna-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "753999dd-9914-4890-91d2-54a0f89fc33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1d output shape: torch.Size([1, 4, 247])\n",
      "ReLU output shape: torch.Size([1, 4, 247])\n",
      "Dropout output shape: torch.Size([1, 4, 247])\n",
      "BatchNorm1d output shape: torch.Size([1, 4, 247])\n",
      "Conv1d output shape: torch.Size([1, 4, 245])\n",
      "ReLU output shape: torch.Size([1, 4, 245])\n",
      "Dropout output shape: torch.Size([1, 4, 245])\n",
      "BatchNorm1d output shape: torch.Size([1, 4, 245])\n",
      "MaxPool1d output shape: torch.Size([1, 4, 122])\n",
      "Conv1d output shape: torch.Size([1, 8, 120])\n",
      "ReLU output shape: torch.Size([1, 8, 120])\n",
      "Dropout output shape: torch.Size([1, 8, 120])\n",
      "BatchNorm1d output shape: torch.Size([1, 8, 120])\n",
      "Conv1d output shape: torch.Size([1, 8, 118])\n",
      "ReLU output shape: torch.Size([1, 8, 118])\n",
      "Dropout output shape: torch.Size([1, 8, 118])\n",
      "MaxPool1d output shape: torch.Size([1, 8, 59])\n",
      "BatchNorm1d output shape: torch.Size([1, 8, 59])\n",
      "Conv1d output shape: torch.Size([1, 8, 57])\n",
      "ReLU output shape: torch.Size([1, 8, 57])\n",
      "Dropout output shape: torch.Size([1, 8, 57])\n",
      "BatchNorm1d output shape: torch.Size([1, 8, 57])\n",
      "Flatten output shape: torch.Size([1, 456])\n",
      "456\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = models.FGANet(\n",
    "    num_input=data.shape[2],\n",
    "    num_output=1,\n",
    "    conv_channels=4,\n",
    "    num_in_channels=data.shape[1])\n",
    "model.eval()\n",
    "# Create a dummy input with the size of [batch_size, channels, width]\n",
    "# where channels is num_in_channels and width is 249\n",
    "dummy_input = torch.randn(1, data.shape[1], 249)\n",
    "\n",
    "with torch.no_grad():\n",
    "    x = dummy_input\n",
    "    for layer in model.features:\n",
    "        x = layer(x)\n",
    "        print(layer.__class__.__name__, \"output shape:\", x.shape)\n",
    "\n",
    "# Flatten the output from the convolutional layers\n",
    "# The view operation reshapes the output to a two-dimensional tensor where the second dimension\n",
    "# is the total number of features from the convolutional output\n",
    "num_dense_input = conv_output.view(conv_output.size(0), -1).size(1)\n",
    "\n",
    "# Now num_dense_input holds the total number of features for the fully connected layer\n",
    "print(num_dense_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29b1a6f7-1e4e-43e7-9a2f-cd349a4a5ff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "249"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be93d055-fedf-420b-976c-c844e9073eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1d output shape: torch.Size([1, 16, 247])\n",
      "ReLU output shape: torch.Size([1, 16, 247])\n",
      "Dropout output shape: torch.Size([1, 16, 247])\n",
      "BatchNorm1d output shape: torch.Size([1, 16, 247])\n",
      "Conv1d output shape: torch.Size([1, 16, 245])\n",
      "ReLU output shape: torch.Size([1, 16, 245])\n",
      "Dropout output shape: torch.Size([1, 16, 245])\n",
      "BatchNorm1d output shape: torch.Size([1, 16, 245])\n",
      "MaxPool1d output shape: torch.Size([1, 16, 122])\n",
      "Conv1d output shape: torch.Size([1, 32, 120])\n",
      "ReLU output shape: torch.Size([1, 32, 120])\n",
      "Dropout output shape: torch.Size([1, 32, 120])\n",
      "BatchNorm1d output shape: torch.Size([1, 32, 120])\n",
      "Conv1d output shape: torch.Size([1, 32, 118])\n",
      "ReLU output shape: torch.Size([1, 32, 118])\n",
      "Dropout output shape: torch.Size([1, 32, 118])\n",
      "MaxPool1d output shape: torch.Size([1, 32, 59])\n",
      "BatchNorm1d output shape: torch.Size([1, 32, 59])\n",
      "Conv1d output shape: torch.Size([1, 32, 57])\n",
      "ReLU output shape: torch.Size([1, 32, 57])\n",
      "Dropout output shape: torch.Size([1, 32, 57])\n",
      "BatchNorm1d output shape: torch.Size([1, 32, 57])\n",
      "Flatten output shape: torch.Size([1, 1824])\n"
     ]
    }
   ],
   "source": [
    "model = models.FGANet(\n",
    "    num_input=data.shape[2],\n",
    "    num_output=1,\n",
    "    conv_channels=args.num_conv,\n",
    "    num_in_channels=data.shape[1],\n",
    "    stride=1,)\n",
    "model.eval()\n",
    "dummy_input = torch.randn(1, data.shape[1], 249)\n",
    "with torch.no_grad():\n",
    "    x = dummy_input\n",
    "    for layer in model.features:\n",
    "        x = layer(x)\n",
    "        print(layer.__class__.__name__, \"output shape:\", x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71c8fc76-54a6-4e7a-ae8c-c41710a0da37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.num_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab4db110-38c7-430f-8943-add35d97c3f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_weights' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(best_weights)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'best_weights' is not defined"
     ]
    }
   ],
   "source": [
    "print(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f98b68b-0cee-4167-9a4f-847dac02111e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
