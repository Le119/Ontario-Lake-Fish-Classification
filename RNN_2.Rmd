---
title: "RNN-input-Alice"
author: "Alice Zhang"
date: '2024-01-29'
output: html_document
---

```{r, warning=FALSE, message=FALSE}
load("~/Downloads/processed_AnalysisData_no200.Rdata")
# ls()
# libraries
library(dplyr)
library(tidyr)
```

```{r}
processed_data_no200%>%group_by(spCode,fishNum)%>%count()
processed_data_no200 <-processed_data_no200 %>%filter(is.na(F100)==F)
# also remove individual LWF23018 (only two pings)
processed_data_no200 <-processed_data_no200 %>%filter(fishNum!="LWF23018")
# remove the individual with -9x10^38 TS
processed_data_no200 <-processed_data_no200[-36830,]
# LT019 and LT23008 were dead the whole time. Remove.
processed_data_no200 <-processed_data_no200 %>%filter(fishNum!="LT019")
processed_data_no200 <-processed_data_no200 %>%filter(fishNum!="LT23008")
# LT015 was at a very shallow depth and on quadrant for the majority of pinging, then seemed to move to the correct depth. Keep only those pings
processed_data_no200 <- processed_data_no200[!(processed_data_no200$fishNum == "LT015" & processed_data_no200$Target_true_depth > 15.5 ) ,]
# LT018 was rough on attachment too, also has two rythmic changes in depth - going to remove the times the fish was above 15.5m as this will get rid of the time that fish was being dragged potentially and the first part of the timeseries where the fish was rough
processed_data_no200 <- processed_data_no200[!(processed_data_no200$fishNum == "LT018" & processed_data_no200$Target_true_depth > 15.5 ) ,]
# LT23018 was rough on attachment, barely alive, but "suprisingly okay" coming back - no clear indication of when it got "okay" in the data so remove whole fish
processed_data_no200 <-processed_data_no200 %>%filter(fishNum!="LT23018")
# LT013 was almost dead on release as well as dead on retrival - remove all.
processed_data_no200 <-processed_data_no200 %>%filter(fishNum!="LT013")
# LT23001 looks like there are barely any salvagable pings - remove all
processed_data_no200 <-processed_data_no200%>%filter(fishNum!="LT23001")
## Cleaned data:
processed_data_no200 %>%filter(spCode==81)%>%group_by(fishNum)%>%count()
# 21 fish, 22,792 pings

## Looking at cleaned LT data
# print(processed_data_no200%>%filter(spCode==81)%>%group_by(fishNum)%>%summarise(TL=mean(totalLength)),n=21)
```

```{r}
## LWT

# 23004 and 23014 both pretty much dead the whole time
processed_data_no200<-processed_data_no200%>%filter(fishNum!="LWF23004")
processed_data_no200<-processed_data_no200%>%filter(fishNum!="LWF23014")

# 23006 and 23008 both "swam" upside down for the majority of the time
processed_data_no200<-processed_data_no200%>%filter(fishNum!="LWF23006")
processed_data_no200<-processed_data_no200%>%filter(fishNum!="LWF23008")

# install.packages("lubridate")
library(lubridate)

LWF011<-processed_data_no200%>%
        filter(fishNum=="LWF011")%>%
        mutate(Ping_time=strptime(Ping_time,format = "%H:%M:%S")-(60*60*4), hour=hour(Ping_time), minute=minute(Ping_time))%>%
  filter(hour==19 | hour==20 & minute <= 9 | hour==20 & minute >= 13)%>%select(c(-hour,-minute))

processed_data_no200<-processed_data_no200%>%filter(fishNum!="LWF011")
processed_data_no200<-rbind(processed_data_no200,LWF011)
# LWF23012 was held at a different depth for the full time, LWF009 was in poor condition on retrieval but activity score 3: keep for now but keep in mind.
```

```{r}
# filter out spCode 102
processed_data_no200<-processed_data_no200%>%filter(spCode!="102")
# remove frequencies 
processed_data_no200<-processed_data_no200%>%select(-F90)
processed_data_no200<-processed_data_no200%>%select(-F90.5)
```

```{r}
# Look at the cleaned processed_data
# processed_data_no200
column_names <- names(processed_data_no200)

# Print the column names
# print(column_names)

# labels for each species
y <- as.factor(processed_data_no200$spCode)
# target strength as X for just one frequency (attempt)
X_170 <- processed_data_no200$F170 # for F170

# function for converting time
convert_to_datetime <- function(time_str) {
  return(as.POSIXct(time_str, format = "%H:%M:%OS"))
}

# Modify the Ping_time column to wanted format
processed_data_no200 <- processed_data_no200 %>%
  mutate(Ping_time = convert_to_datetime(Ping_time))
```

# Averaging Pings (1s)
```{r}
# averaging TS for pings within each 1 sec interval, generate new pings
data_1sec <- processed_data_no200 %>%
  arrange(fishNum, Ping_time) %>%
  group_by(fishNum) %>%
  filter(!is.na(as.numeric(Ping_time))) %>%  # Exclude rows with non-numeric Ping_time
  mutate(Ping_time = as.numeric(Ping_time),  # Convert Ping_time to numeric
         diff = Ping_time - lag(Ping_time, default = first(Ping_time))) %>%
  mutate(region = cumsum(diff > 1)) %>%
  ungroup() %>%
  group_by(fishNum, region, .add = TRUE) %>%
  filter(all(!is.na(diff), diff <= 1))  # Exclude rows with NAs in diff and diff > 1

# Create data frames for each region
### here, region means each average taken (aka every 1 sec coverage) - not necessary the new fish*
region_data_frames <- data_1sec %>%
  # group_split(fishNum, region)
  group_by(fishNum, region, .add=TRUE)%>%group_split()


# Print data frames for each region
for (i in seq_along(region_data_frames)) {
  cat("Region", i, ":\n")
  print(region_data_frames[[i]])
  cat("\n")
}

# create a dataframe, column1: the mean of the column F170 across the rows for each region, column2: the spCode, column 3 fishNum, column4: fishID (-original)
```

# Averaging Pings (0.9s)
```{r}
# averaging TS for pings within each 0.9 sec interval, generate new pings
data_09sec <- processed_data_no200 %>%
  arrange(fishNum, Ping_time) %>%
  group_by(fishNum) %>%
  filter(!is.na(as.numeric(Ping_time))) %>%  # Exclude rows with non-numeric Ping_time
  mutate(Ping_time = as.numeric(Ping_time),  # Convert Ping_time to numeric
         diff = Ping_time - lag(Ping_time, default = first(Ping_time))) %>%
  mutate(region = cumsum(diff > 0.9)) %>%
  ungroup() %>%
  group_by(fishNum, region, .add = TRUE) %>%
  filter(all(!is.na(diff), diff <= 0.9))  # Exclude rows with NAs in diff and diff > 0.9

# # Create data frames for each region
# ### here, region means each average taken (aka every 1 sec coverage) - not necessary the new fish*
region_data_frames <- data_09sec %>%
  # group_split(fishNum, region)
  group_by(fishNum, region, .add=TRUE)%>%group_split()
```


```{r}
### 1
# Assuming you want to store the results in a list
result_list <- list()

for (i in seq_along(region_data_frames)) {
  current_region_data <- region_data_frames[[i]]

  # Calculate the mean of the column F170 for each region
  mean_F170 <- current_region_data %>%
    summarise(mean_F170 = mean(F170, na.rm = TRUE))

  # Extract spCode and fishNum for each region
  spCode_fish <- current_region_data %>%
    select(spCode) %>%
    distinct()  # Assuming each region has the same spCode

  # Combine the results into a single dataframe
  region_result <- cbind(mean_F170, spCode_fish)

  # Append the result to the list
  result_list[[i]] <- region_result
}

# Print the list of dataframes
for (i in seq_along(result_list)) {
  cat("Region", i, ":\n")
  print(result_list[[i]])
  cat("\n")
}

### 2
# Assuming you want to store the results in a list
result_list <- list()

for (i in seq_along(region_data_frames)) {
  current_region_data <- region_data_frames[[i]]

  # Remove specified columns
  processed_data_no_cols <- current_region_data %>%
    select(-fishNum, -forkLength, -FishTrack, -Fish_track_change_in_range, -Fish_track_change_in_depth)

  # Select columns that start with F followed by an integer
  selected_columns <- processed_data_no_cols %>%
    select(matches("^F\\d+$")) %>%
    mutate(across(everything(), as.numeric))

  # Calculate the mean of the selected columns for each region
  mean_frequencies <- processed_data_no_cols %>%
    summarise(across(starts_with("F"), mean, na.rm = TRUE))

  # Extract spCode for each region
  spCode <- processed_data_no_cols %>%
    distinct(spCode)

  # Combine the results into a single dataframe
  region_result <- cbind(mean_frequencies, spCode)

  # Append the result to the list
  result_list[[i]] <- region_result

  # Save the result as a CSV file (adjust the file path as needed)
  write.csv(region_result, file = paste0("region_", i, "_result.csv"), row.names = FALSE)
}

# Print the list of dataframes (optional)
for (i in seq_along(result_list)) {
  cat("Region", i, ":\n")
  print(result_list[[i]])
  cat("\n")
}


library(dplyr)

# Assuming you want to store the results in a list
result_list <- list()

for (i in seq_along(region_data_frames)) {
  current_region_data <- region_data_frames[[i]]

  # Remove specified columns
  processed_data_no_cols <- current_region_data %>%
    select(-fishNum, -forkLength, -FishTrack, -Fish_track_change_in_range, -Fish_track_change_in_depth)

  # Select columns that start with F followed by an integer
  selected_columns <- processed_data_no_cols %>%
    select(matches("^F\\d+$")) %>%
    mutate(across(everything(), as.numeric))

  # Calculate the mean of the selected columns for each region
  mean_frequencies <- processed_data_no_cols %>%
    summarise(across(starts_with("F"), mean, na.rm = TRUE))

  # Extract spCode for each region
  spCode <- processed_data_no_cols %>%
    distinct(spCode)

  # Combine the results into a single dataframe
  region_result <- cbind(mean_frequencies, spCode)

  # Append the result to the list
  result_list[[i]] <- region_result
}

# Print the list of dataframes
for (i in seq_along(result_list)) {
  cat("Region", i, ":\n")
  print(result_list[[i]])
  cat("\n")
}


# for each region, save the values of the frequencies and the spcode

# from F45 to F170, we have frequcies like F45, F45.5, F50,....F169.5, F170, I want to # create a dataframe, column1: the mean of the columns of the frequencies across the rows for each region, column2: the spCode
library(dplyr)

# Assuming you want to store the results in a list
result_list <- list()

for (i in seq_along(region_data_frames)) {
  current_region_data <- region_data_frames[[i]]

  # Select columns F45 to F170
  selected_columns <- select(current_region_data, starts_with("F")) %>% # start with "F" and also follow with integer
    select(F45:F170)

  # Calculate the mean of the selected columns for each region
  mean_frequencies <- current_region_data %>%
    summarise(across(starts_with("F"), mean, na.rm = TRUE))

  # Extract spCode for each region
  spCode <- current_region_data %>%
    distinct(spCode)

  # Combine the results into a single dataframe
  region_result <- cbind(mean_frequencies, spCode)

  # Append the result to the list
  result_list[[i]] <- region_result
}

# Print the list of dataframes
for (i in seq_along(result_list)) {
  cat("Region", i, ":\n")
  print(result_list[[i]])
  cat("\n")
}
```

# Grouping pairs of pings into one region, generate new fish
```{r}


# library(purrr)
# 
# new_data_1sec <- data_1sec %>%
#   group_by(fishNum) %>% # group pings by fishNum
#   arrange(Ping_time) %>% # arrange by ping time
#   group_by(keep.unused_groups = TRUE, .add = TRUE) %>% group_split() %>% # data split into groups based on fishNum
#   map(function(group) { # create pairs in each fishNum, if odd remove last (problem: if only one ping after averaging, will lose entire fish)
#     n <- nrow(group)
#     if (n %% 2 != 0) {
#       group <- group[-n, ]  # Remove the last row if the number of rows is odd
#     }
#     pair1 <- group[c(TRUE, FALSE), ] # to select alternate rows
#     pair2 <- group[c(FALSE, TRUE), ]
#     bind_rows(pair1, pair2)
#   }) %>%
#   bind_rows() %>%
#   ungroup()

```

```{r}
length(processed_data_no200)
# processed_data_no200

length(data_1sec)
# processed_data_no200_1sec

# LWT
p1_processed_data_no200_LWT <- p1_processed_data_no200 %>%
  filter(spCode == 91)

# LT
p1_processed_data_no200_LT <- p1_processed_data_no200 %>%
  filter(spCode == 81)

# SMB
p1_processed_data_no200_LT <- p1_processed_data_no200 %>%
  filter(spCode == 316)
```
```{r}
F45index <- which(names(data_09sec) == "F45")
f1=seq(from=45,to=89.5,by=0.5) #first group of frequencies
listf1=seq(from=F45index,to=(F45index-1)+length(f1),by=1) #columns identifying the first group of frequencies
```


# RNN Package
https://github.com/bquast/rnn?tab=readme-ov-file
```{r}
# install.packages('rnn')
library(rnn)

help('trainr')
help('predictr')
help(package='rnn')

set.seed(1)
train.id = sample(1:length(data_09sec$region), 0.7*length(data_09sec$region))
df.train = data_09sec[train.id,]
df.test = data_09sec[-train.id,]

x.train = array(cbind(df.train[,2], df.train[,listf1]), 
                dim = c(length(train.id), length(df.train[1,]), 1)) # only take fishNum and TS info
y.train = array(df.train$spCode, dim = dim(x.train)) # take species label


trainr(Y=y.train, X=x.train, batch_size=1, learningrate=0.1)
```

https://www.kaggle.com/code/rtatman/beginner-s-intro-to-rnn-s-in-r
```{r}
# read in the packages we'll use
library(keras) # for deep learning
library(tensorflow)
library(tidyverse) # general utility functions
library(caret) # machine learning utility functions

batch_size <- 32 # number of sequences to look at at one time during training
total_epochs <- 15 # how many times we'll look @ the whole dataset while training our model

# set a random seed for reproducability
set.seed(1)
rnn = keras_model_sequential() # initialize model
# our input layer
rnn %>%
    layer_dense(input_shape = dim(x.train)[2:3], units = length(df.train[1,]))
rnn %>%
    layer_dense(units = 1, activation = 'sigmoid') # output
# look at our model architecture
summary(rnn)
rnn %>% compile(loss = 'binary_crossentropy', 
                  optimizer = 'RMSprop', 
                  metrics = c('accuracy'))
```

```{r}
# Actually train our model! This step will take a while
trained_model <- model %>% fit(
    x = X_train, # sequence we're using for prediction 
    y = y_train, # sequence we're predicting
    batch_size = batch_size, # how many samples to pass to our model at a time
    epochs = total_epochs, # how many times we'll look @ the whole dataset
    validation_split = 0.1) # how much data to hold out for testing as we go along
```


# RNN Implementation
```{r}
# # Install the required packages
# install.packages("keras")
# install.packages("tensorflow")
# 
# # Load the installed packages
# library(keras)
# library(tensorflow)
# 
# # RNN Implementation
# 
# # prompt: create (x, y) for the input to RNN, x should be the TS_mean_2sec, y should be the species, the RNN is to conduct binary classification, also add the implementation of RNN
# 
# # Reshape the input data
# input_data <- array_reshape(p1_processed_data_no200_LWT$TS_mean_2sec, c(dim(p1_processed_data_no200_LWT$TS_mean_2sec)[1], 1, 1))
# 
# # Define the model
# rnn <- keras_model_sequential()
# rnn <- rnn %>%
#   layer_lstm(units = 128, return_sequences = TRUE, input_shape = c(1, 1)) %>%
#   layer_lstm(units = 128) %>%
#   layer_dense(units = 1, activation = 'sigmoid')
# 
# # Compile the model
# rnn %>%
#   compile(optimizer = 'adam',
#           loss = 'binary_crossentropy',
#           metrics = c('accuracy'))
# 
# # Train the model
# rnn %>%
#   fit(x = input_data,
#       y = p1_processed_data_no200_LWT$spCode,
#       epochs = 100,
#       batch_size = 32)
# 
# # Evaluate the model
# rnn %>%
#   evaluate(x = input_data,
#            y = p1_processed_data_no200_LWT$spCode)
# 
# 
# 
# ### Previous methods to calculate time difference
# 
# LT <- processed_data[processed_data$spCode == 81, ]
# 
# # Function to convert time string to POSIXct
# convert_to_datetime <- function(time_str) {
#   return(as.POSIXct(time_str, format = "%H:%M:%OS"))
# }
# 
# # Function to calculate the time difference for every 5 rows
# calculate_time_difference_for_five_rows <- function(data) {
#   time_differences <- numeric(0)
# 
#   for (i in seq(1, nrow(data), by = 5)) {
#     if ((i + 4) <= nrow(data)) {
#       time_str1 <- data$Ping_time[i]
#       time_str2 <- data$Ping_time[i + 4]
# 
#       time1 <- convert_to_datetime(time_str1)
#       time2 <- convert_to_datetime(time_str2)
# 
#       time_difference <- as.numeric(difftime(time1, time2, units = "secs"))
#       time_differences <- c(time_differences, time_difference)
#     }
#   }
# 
#   return(time_differences)
# }
# 
# # Calculate time differences for every 5 rows
# time_differences <- calculate_time_difference_for_five_rows(LT)
# 
# groups <- cumsum(abs(time_differences) >= 5)
# 
# 
# # Run-length encoding to calculate the length of each region
# lengths_of_regions <- rle(groups)$lengths
# 
# # Print the lengths of each region
# print(lengths_of_regions)
# 
# # choose 5 ping for each region, doesn't have to be same time interval
# 
# LWT <- processed_data[processed_data$spCode == 91, ]
# 
# # Function to convert time string to POSIXct
# convert_to_datetime <- function(time_str) {
#   return(as.POSIXct(time_str, format = "%H:%M:%OS"))
# }
# 
# # Function to calculate the time difference for every 5 rows
# calculate_time_difference_for_five_rows <- function(data) {
#   time_differences <- numeric(0)
# 
#   for (i in seq(1, nrow(data), by = 5)) {
#     if ((i + 4) <= nrow(data)) {
#       time_str1 <- data$Ping_time[i]
#       time_str2 <- data$Ping_time[i + 4]
# 
#       time1 <- convert_to_datetime(time_str1)
#       time2 <- convert_to_datetime(time_str2)
# 
#       time_difference <- as.numeric(difftime(time1, time2, units = "secs"))
#       time_differences <- c(time_differences, time_difference)
#     }
#   }
# 
#   return(time_differences)
# }
# 
# # Calculate time differences for every 5 rows
# time_differences <- calculate_time_difference_for_five_rows(LT)
# 
# groups <- cumsum(abs(time_differences) >= 5)
# 
# 
# # Run-length encoding to calculate the length of each region
# lengths_of_regions <- rle(groups)$lengths
# 
# # Print the lengths of each region
# print(lengths_of_regions)
# 
# Ping_time_number <- processed_data[, c("Ping_time", "pingNumber")]
# 
# # Function to convert time string to POSIXct
# convert_to_datetime <- function(time_str) {
#   return(as.POSIXct(time_str, format = "%H:%M:%OS"))
# }
# 
# # I wanna calculate the time difference for every 5 pingNumber
# calculate_time_difference <- function(time_str1, time_str2) {
# 
#   time1 <- convert_to_datetime(time_str1)
#   time2 <- convert_to_datetime(time_str2)
# 
# 
#   time_difference <- abs(difftime(time1, time2))
# 
#   return(time_difference)
# }
# 
# print(paste("Time difference:", as.numeric(time_difference), "seconds"))
# 
# 
# ### frequencies as the columns
# ### Take the average over the target strength across the frequencies
```

